{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sophia HPC cluster This website is a resource with technical details for users to make efficient use of the computing and storage resources of the Technical University of Denmark's high performance computing cluster Sophia . The Sophia HPC cluster runs the Linux CentOS operating system and the cluster installation is based on the OpenHPC framework. The default shell is bash and users are encouraged to use bash since Sophia's compute job scheduler primarily supports this shell. Acknowledgment of use Please use DOI 10.57940/FAFC-6M81 in your publication; select citation format here https://doi.datacite.org/dois/10.57940%2Ffafc-6m81 , and e.g. use the following text as appropriate: The authors gratefully acknowledge the computational and data resources provided on the Sophia HPC Cluster at the Technical University of Denmark, DOI: 10.57940/FAFC-6M81. Guidelines for use Through constructive dialogue we wish on one hand to ensure continuity and progress in on-going research activities and on the other hand to promote the general use of HPC in research at DTU and other Danish universities. Optimal use of the HPC resources available requires responsible and conscientious use from the individual user. Read these docs pages! When in doubt, ask! Write the address at the bottom left. The HPC resources can be used for development and testing of applications; please ensure that reserved HPC resources are utilised and not standing idle. E.g. do not reserve multiple nodes for code that only utilise resources on a single node. Carefully consider the impact of reserving several nodes on other users' ability to carry out their work; e.g. inspect current load prior to job submission, e.g. postpone resource-demanding compute jobs until evening, etc. Please strive to reserve and utilise full compute nodes to the extent feasible. Do not run code on the login node! Request an interactive job . Jobs or login node usage that negatively impacts general use will be stopped and access for the offending user may be restricted or denied if this occurs repeatedly. The decision of restriction of access or closing an account is taken in consensus between the DTU IT Department, DTU Wind Energi and DTU Mechanical Engineering, each having one representative.","title":"Sophia HPC cluster"},{"location":"#sophia-hpc-cluster","text":"This website is a resource with technical details for users to make efficient use of the computing and storage resources of the Technical University of Denmark's high performance computing cluster Sophia . The Sophia HPC cluster runs the Linux CentOS operating system and the cluster installation is based on the OpenHPC framework. The default shell is bash and users are encouraged to use bash since Sophia's compute job scheduler primarily supports this shell.","title":"Sophia HPC cluster"},{"location":"#acknowledgment-of-use","text":"Please use DOI 10.57940/FAFC-6M81 in your publication; select citation format here https://doi.datacite.org/dois/10.57940%2Ffafc-6m81 , and e.g. use the following text as appropriate: The authors gratefully acknowledge the computational and data resources provided on the Sophia HPC Cluster at the Technical University of Denmark, DOI: 10.57940/FAFC-6M81.","title":"Acknowledgment of use"},{"location":"#guidelines-for-use","text":"Through constructive dialogue we wish on one hand to ensure continuity and progress in on-going research activities and on the other hand to promote the general use of HPC in research at DTU and other Danish universities. Optimal use of the HPC resources available requires responsible and conscientious use from the individual user. Read these docs pages! When in doubt, ask! Write the address at the bottom left. The HPC resources can be used for development and testing of applications; please ensure that reserved HPC resources are utilised and not standing idle. E.g. do not reserve multiple nodes for code that only utilise resources on a single node. Carefully consider the impact of reserving several nodes on other users' ability to carry out their work; e.g. inspect current load prior to job submission, e.g. postpone resource-demanding compute jobs until evening, etc. Please strive to reserve and utilise full compute nodes to the extent feasible. Do not run code on the login node! Request an interactive job . Jobs or login node usage that negatively impacts general use will be stopped and access for the offending user may be restricted or denied if this occurs repeatedly. The decision of restriction of access or closing an account is taken in consensus between the DTU IT Department, DTU Wind Energi and DTU Mechanical Engineering, each having one representative.","title":"Guidelines for use"},{"location":"access/","text":"How to access HPC resources DTU Sophia is accessed via the Secure Shell protocol and hence a ssh client is needed on the user's laptop from which connection is to be established to DTU Sophia. This is preferably installed via the laptop operating system's software package manager. When outside DTU network DTU Employee Use your laptop's ssh client from a terminal or create a virtual private network (VPN) connection. Tool URL Example ssh client; e.g. OpenSSH ssh.risoe.dk ssh my-dtu-username@ssh.risoe.dk vpn client; e.g. OpenConnect (Linux) or AnyConnect (Windows) vpn.dtu.dk openconnect --os=win vpn.dtu.dk DeiC grant holder Log on the remote portal and choose DeiC Sophia Desktop - this will open a Citrix session with Linux CentOS 7. Accessing Sophia login node Linux Open a terminal and access Sophia from the commandline; ssh <username>@sophia.dtu.dk Microsoft Windows The MobaXterm Windows application offers a range of features, including e.g. default graphics forwarding. Use the same command as for Linux.","title":"How to access"},{"location":"access/#how-to-access-hpc-resources","text":"DTU Sophia is accessed via the Secure Shell protocol and hence a ssh client is needed on the user's laptop from which connection is to be established to DTU Sophia. This is preferably installed via the laptop operating system's software package manager.","title":"How to access HPC resources"},{"location":"access/#when-outside-dtu-network","text":"","title":"When outside DTU network"},{"location":"access/#dtu-employee","text":"Use your laptop's ssh client from a terminal or create a virtual private network (VPN) connection. Tool URL Example ssh client; e.g. OpenSSH ssh.risoe.dk ssh my-dtu-username@ssh.risoe.dk vpn client; e.g. OpenConnect (Linux) or AnyConnect (Windows) vpn.dtu.dk openconnect --os=win vpn.dtu.dk","title":"DTU Employee"},{"location":"access/#deic-grant-holder","text":"Log on the remote portal and choose DeiC Sophia Desktop - this will open a Citrix session with Linux CentOS 7.","title":"DeiC grant holder"},{"location":"access/#accessing-sophia-login-node","text":"","title":"Accessing Sophia login node"},{"location":"access/#linux","text":"Open a terminal and access Sophia from the commandline; ssh <username>@sophia.dtu.dk","title":"Linux"},{"location":"access/#microsoft-windows","text":"The MobaXterm Windows application offers a range of features, including e.g. default graphics forwarding. Use the same command as for Linux.","title":"Microsoft Windows"},{"location":"account/","text":"Getting a user account In order to access HPC resources a user account is necessary. DTU employee Students must have a DTU staff member (e.g. supervisor) forward user account requests on their behalf in order to access DTU Sophia resources. DTU staff must direct account creation requests to the address at the bottom left. DeiC grant holder A DeiC grant holder can request access to DTU Sophia resources by forwarding - to the address at the bottom left - correspondence with the relevant user creation approver. Until more elaborate guidelines are finalised, Dan Ariel S\u00f8ndergaard is acting approver for DTU Sophia resource requests from employees at other Danish universities.","title":"User account"},{"location":"account/#getting-a-user-account","text":"In order to access HPC resources a user account is necessary.","title":"Getting a user account"},{"location":"account/#dtu-employee","text":"Students must have a DTU staff member (e.g. supervisor) forward user account requests on their behalf in order to access DTU Sophia resources. DTU staff must direct account creation requests to the address at the bottom left.","title":"DTU employee"},{"location":"account/#deic-grant-holder","text":"A DeiC grant holder can request access to DTU Sophia resources by forwarding - to the address at the bottom left - correspondence with the relevant user creation approver. Until more elaborate guidelines are finalised, Dan Ariel S\u00f8ndergaard is acting approver for DTU Sophia resource requests from employees at other Danish universities.","title":"DeiC grant holder"},{"location":"hardware/","text":"DTU Sophia hardware Compute nodes The Sophia HPC cluster consists of 516 computational nodes of which 484 are 128 GB RAM nodes and 32 are 256 GB RAM nodes. Each node is a powerful x86-64 computer, equipped with 32 physical cores (2 x sixteen-core AMD EPYC 7351). The parameters are summarized in the following table: Specs Primary purpose High Performance Computing Architecture of compute nodes x86-64 Operating system CentOS 7 Linux Compute nodes in total 516 Processor 2 x AMD EPYC 7351, 2.9 GHz, 16 cores RAM (484 nodes) 128 GB, 4 GB per core, DDR4@2666 MHz RAM (32 nodes) 256 GB, 8 GB per core, DDR4@2666 MHz Local disk drive no Compute network / Topology InfiniBand EDR / Fat tree In total Total theoretical peak performance (Rpeak) ~384 TFLOPS (516 nodes x 32 cores x 2.9GHz x 8 FLOP/cycle) Total amount of RAM 69 TB High-speed interconnect The nodes are interlinked by InfiniBand and 10 Gbps Ethernet networks. Sophia's high-speed, low-latency interconnect is Mellanox EDR (100Gbps) Infiniband. Frontend-, compute-, and burst buffer nodes each have Mellanox' ConnectX-5 adapter card installed. Switch system Count SB7700 2 SB7790 47 Burst buffer Hardware product Count Dell R7425 w/NVMe front-bay 2 Dell Express Flash PM1725a 1.6TB 16 Dell Express Flash PM1725b 1.6TB 4","title":"Compute hardware"},{"location":"hardware/#dtu-sophia-hardware","text":"","title":"DTU Sophia hardware"},{"location":"hardware/#compute-nodes","text":"The Sophia HPC cluster consists of 516 computational nodes of which 484 are 128 GB RAM nodes and 32 are 256 GB RAM nodes. Each node is a powerful x86-64 computer, equipped with 32 physical cores (2 x sixteen-core AMD EPYC 7351). The parameters are summarized in the following table: Specs Primary purpose High Performance Computing Architecture of compute nodes x86-64 Operating system CentOS 7 Linux Compute nodes in total 516 Processor 2 x AMD EPYC 7351, 2.9 GHz, 16 cores RAM (484 nodes) 128 GB, 4 GB per core, DDR4@2666 MHz RAM (32 nodes) 256 GB, 8 GB per core, DDR4@2666 MHz Local disk drive no Compute network / Topology InfiniBand EDR / Fat tree In total Total theoretical peak performance (Rpeak) ~384 TFLOPS (516 nodes x 32 cores x 2.9GHz x 8 FLOP/cycle) Total amount of RAM 69 TB","title":"Compute nodes"},{"location":"hardware/#high-speed-interconnect","text":"The nodes are interlinked by InfiniBand and 10 Gbps Ethernet networks. Sophia's high-speed, low-latency interconnect is Mellanox EDR (100Gbps) Infiniband. Frontend-, compute-, and burst buffer nodes each have Mellanox' ConnectX-5 adapter card installed. Switch system Count SB7700 2 SB7790 47","title":"High-speed interconnect"},{"location":"hardware/#burst-buffer","text":"Hardware product Count Dell R7425 w/NVMe front-bay 2 Dell Express Flash PM1725a 1.6TB 16 Dell Express Flash PM1725b 1.6TB 4","title":"Burst buffer"},{"location":"modules/","text":"Software modules List available modules ml avail and load a particular module, e.g. ml OpenMPI/3.1.1-GCC-7.3.0-2.30 to use OpenMPI version 3.1.1 compiled with GNU Compiler Collection version 7.3. To get an overview of modules currently loaded in the shell environment do ml Unload a particular module with ml unload <module> , e.g. ml unload GCC/7.3.0-2.30 or just ml unload GCC will remove the GNU Compiler Collection from the environment (and OpenMPI would stop functioning!). Installing software As an alternative to building software manually a HPC package manager can be used. EasyBuild Sophia users can load the EasyBuild framework with ml EasyBuild and e.g. go through a workflow example to get the hang of it. An alphabetically sorted list of software recipies - so-called easyconfigs - are available on GitHub . Spack Another option is the Spack package manager ; we refer to the docs for further information.","title":"Software modules"},{"location":"modules/#software-modules","text":"List available modules ml avail and load a particular module, e.g. ml OpenMPI/3.1.1-GCC-7.3.0-2.30 to use OpenMPI version 3.1.1 compiled with GNU Compiler Collection version 7.3. To get an overview of modules currently loaded in the shell environment do ml Unload a particular module with ml unload <module> , e.g. ml unload GCC/7.3.0-2.30 or just ml unload GCC will remove the GNU Compiler Collection from the environment (and OpenMPI would stop functioning!).","title":"Software modules"},{"location":"modules/#installing-software","text":"As an alternative to building software manually a HPC package manager can be used.","title":"Installing software"},{"location":"modules/#easybuild","text":"Sophia users can load the EasyBuild framework with ml EasyBuild and e.g. go through a workflow example to get the hang of it. An alphabetically sorted list of software recipies - so-called easyconfigs - are available on GitHub .","title":"EasyBuild"},{"location":"modules/#spack","text":"Another option is the Spack package manager ; we refer to the docs for further information.","title":"Spack"},{"location":"permanent/","text":"Long term storage Ceph file system The file systems mounted under /home and /groups are ceph file systems. Your home folder is for configuration files and personal data, for example, ssh-keys, local copies of code for development, publication drafts and referee reports. Never store any data intended for sharing in your home folder. Group folders are for data shared within a group of researchers, for example, members of a department or section, members of a project or users of a licensed or otherwise restricted software. Group folders should be used for storing all research data, including data resulting from master-, Ph.D.- and post-doc projects. Supervisors of such projects should ask for creation of a group folder for this purpose. Furthermore, these two file systems are intended for storing warm and cold data but not hot or temporary data: Cold data (archival data): Data with a long life-time (years to decades to forever). This data is typically immutable and non-reproducible. Warm data (work in progress data): Data with intermediate life-time (days to weeks to months). Some of this data will become cold data, others will be deleted at the end of a project or after publication. Hot data (job working set): Data with a life-time of a job execution. This is all data that is required during a run of a job, but has no purpose after job completion. Never save hot data on the ceph file systems! This produces unnecessary load on the shared file systems and will pollute the ceph snapshots. Please use the burst buffer or the local RAM disk for hot and temporary data. Performance considerations The ceph filesystem is optimized for capacity, data protection, cost and sequential throughput - in this order. In addition, ceph is not a parallel file system like, for example, Lustre or BeeGFS. This has important implications on performance and how to run jobs using the various file systems in the best possible way. Ideal work flow The ceph file system is designed for best performance with a temporary scratch workflow: Copy multi-access data to temporary storage (burst buffer or RAM disk). Ideally, this is an extraction from a single large archive (tar, hdf5, zip, ...) directly to temporary storage, which will benefit from the high streaming bandwidth of ceph. Data accessed only once by an application should be directly read from ceph. Ideally, access is sequential from large files. During job execution, all new data is produced on temporary storage. Use the burst buffer if parallel multi-node access is required, or the RAM disk for single-node access. RAM disk is the fastest option but provides also the smallest amount of temporary storage capacity. At the end, copy all permanent data back to ceph. Ideally, this is a creation of an archive (tar, hdf5, zip, ...) directly to ceph, which will again benefit from the high streaming bandwidth of ceph. You can use a simplified workflow if your application sequentially reads some (large) files only once, does not require any disk access during computation and writes some (large) files sequentially after completion. Here, 'some' is a small number, like 5-10 files but not hundreds or thousands. Applications with this access pattern should run directly on the ceph file systems. Performance characteristics, Do's and Dont's Our ceph file systems are optimized for large-file I/O. Creating, copying, deleting, listing large numbers of small files is slow. Ideally, use archival data formats like tar or hdf5 on ceph. provide ca. 2000 aggregated IOP/s (random 4K writes) and an aggregated bandwidth of ca. 4.7GB/s (sequential 1M writes). reach link speed (ca. 850MB/s) with single node sequential 1M writes. Differently to the Lustre file system used on Sophia predecessor systems, the ceph file system is not a parallel file system. Furthermore, the ceph file system is not a low-latency storage system either, the latency is of the order of 1ms. The ceph file system is designed for cloud applications, where every client (VM, user, app) has exclusive access to its own directory sub-tree. In other words, no two clients access files in the same directory sub-tree. While ceph supports multi-client (multi-node) access to files in the same directory sub-tree, this access is not necessarily concurrent as with parallel file systems. Rather, the ceph storage cluster serializes concurrent access whenever necessary to present consistent data and meta-data information across all clients. In addition, ceph maintains cache coherency between clients, meaning that a write on one node can invalidate read cache on another node. These characteristics can result in an counter-intuitive experience when transitioning from a parallel file system to a ceph file system. Most of the pitfalls can be avoided by following our ideal workflow outlined above. We hope this \"Do's and Dont's\" list helps avoiding others: Do check this best practices guide for good and bad workloads. Don't use multi-node concurrent file access . Do collection of data on the job's master node and write data to disk only from the master node. Don't send a ticket just because this ls -l doesn't finish in 5s. A slow response is almost certainly not indicating a malfunction of the ceph cluster, but rather a bad workload like concurrent access of a directory sub-tree. Don't run a job and check its progress with ls -l on the head node. This creates a concurrent access with all the synchronization overhead. It will not only be slow on the head node, it will also stall the job. Do ssh/mrsh into the master node of the job and run your ls -l there (i.e. use single-node access). Don't submit a sequence of jobs using the same application. After each job the SLURM prologue script flushes the file system cache on the compute nodes. To avoid reloading the same data from disk for every computation, do run a sequence of computations with the same application in one job. This is particularly important for applications that open thousands of files, like Matlab. Use srun to execute single-application multiple-input jobs in an efficient way. Don't call fflush and fsync , sync excessively, for example, after every single write. Let writes accummulate in the system buffer and call one of these functions after at least 4MB worth of writes are processed. Do use fclose() after writes are completed to ensure data is committed . Snapshots In case of accidental deletion, corruption or modification of data by malware, previous versions are available in ceph snapshots. To access a snapshot of a file, change to the file's directory and execute cd .snap . Note that \".snap\" is a meta-directory, not a real directory. Therefore, tab-completion and the like will not work and one needs to type it explicitly as stated here. Inside the snapshot directory are folders with current snapshots, for example, [frans@sophia1 .snap]$ ls _2021-02-23_183554+0100_weekly_1099511719926 _2021-02-27_000611+0100_daily_1099511719926 _2021-02-24_000611+0100_daily_1099511719926 _2021-02-28_000611+0100_daily_1099511719926 _2021-02-25_000611+0100_daily_1099511719926 _2021-03-01_000611+0100_daily_1099511719926 _2021-02-26_000611+0100_daily_1099511719926 _2021-03-01_000911+0100_weekly_1099511719926 To access snapshots of a specific date, cd into the corresponding folder and browse through the files. Note that files in snapshots are read-only regardless of the file permissions shown by ls -l . Disclaimer Snapshots are provided as a courtesy service on the ceph file systems /home and /groups . The availability of snapshots depends on the amount of currently unused ceph storage capacity and the retention time may be reduced without warning to adjust to increased utilisation. The default retention scheme is: daily snapshots for 7 days and weekly snapshots for 4 weeks. We cannot guarantee snapshots to be present in situations where excessive amounts of temporary data have been captured in snapshots. We aim, however, to have at least the last two days available. Advanced topics LazyIO provided by libcephfs Ceph fs and POSIX .","title":"Storing"},{"location":"permanent/#long-term-storage","text":"","title":"Long term storage"},{"location":"permanent/#ceph-file-system","text":"The file systems mounted under /home and /groups are ceph file systems. Your home folder is for configuration files and personal data, for example, ssh-keys, local copies of code for development, publication drafts and referee reports. Never store any data intended for sharing in your home folder. Group folders are for data shared within a group of researchers, for example, members of a department or section, members of a project or users of a licensed or otherwise restricted software. Group folders should be used for storing all research data, including data resulting from master-, Ph.D.- and post-doc projects. Supervisors of such projects should ask for creation of a group folder for this purpose. Furthermore, these two file systems are intended for storing warm and cold data but not hot or temporary data: Cold data (archival data): Data with a long life-time (years to decades to forever). This data is typically immutable and non-reproducible. Warm data (work in progress data): Data with intermediate life-time (days to weeks to months). Some of this data will become cold data, others will be deleted at the end of a project or after publication. Hot data (job working set): Data with a life-time of a job execution. This is all data that is required during a run of a job, but has no purpose after job completion. Never save hot data on the ceph file systems! This produces unnecessary load on the shared file systems and will pollute the ceph snapshots. Please use the burst buffer or the local RAM disk for hot and temporary data.","title":"Ceph file system"},{"location":"permanent/#performance-considerations","text":"The ceph filesystem is optimized for capacity, data protection, cost and sequential throughput - in this order. In addition, ceph is not a parallel file system like, for example, Lustre or BeeGFS. This has important implications on performance and how to run jobs using the various file systems in the best possible way.","title":"Performance considerations"},{"location":"permanent/#ideal-work-flow","text":"The ceph file system is designed for best performance with a temporary scratch workflow: Copy multi-access data to temporary storage (burst buffer or RAM disk). Ideally, this is an extraction from a single large archive (tar, hdf5, zip, ...) directly to temporary storage, which will benefit from the high streaming bandwidth of ceph. Data accessed only once by an application should be directly read from ceph. Ideally, access is sequential from large files. During job execution, all new data is produced on temporary storage. Use the burst buffer if parallel multi-node access is required, or the RAM disk for single-node access. RAM disk is the fastest option but provides also the smallest amount of temporary storage capacity. At the end, copy all permanent data back to ceph. Ideally, this is a creation of an archive (tar, hdf5, zip, ...) directly to ceph, which will again benefit from the high streaming bandwidth of ceph. You can use a simplified workflow if your application sequentially reads some (large) files only once, does not require any disk access during computation and writes some (large) files sequentially after completion. Here, 'some' is a small number, like 5-10 files but not hundreds or thousands. Applications with this access pattern should run directly on the ceph file systems.","title":"Ideal work flow"},{"location":"permanent/#performance-characteristics-dos-and-donts","text":"Our ceph file systems are optimized for large-file I/O. Creating, copying, deleting, listing large numbers of small files is slow. Ideally, use archival data formats like tar or hdf5 on ceph. provide ca. 2000 aggregated IOP/s (random 4K writes) and an aggregated bandwidth of ca. 4.7GB/s (sequential 1M writes). reach link speed (ca. 850MB/s) with single node sequential 1M writes. Differently to the Lustre file system used on Sophia predecessor systems, the ceph file system is not a parallel file system. Furthermore, the ceph file system is not a low-latency storage system either, the latency is of the order of 1ms. The ceph file system is designed for cloud applications, where every client (VM, user, app) has exclusive access to its own directory sub-tree. In other words, no two clients access files in the same directory sub-tree. While ceph supports multi-client (multi-node) access to files in the same directory sub-tree, this access is not necessarily concurrent as with parallel file systems. Rather, the ceph storage cluster serializes concurrent access whenever necessary to present consistent data and meta-data information across all clients. In addition, ceph maintains cache coherency between clients, meaning that a write on one node can invalidate read cache on another node. These characteristics can result in an counter-intuitive experience when transitioning from a parallel file system to a ceph file system. Most of the pitfalls can be avoided by following our ideal workflow outlined above. We hope this \"Do's and Dont's\" list helps avoiding others: Do check this best practices guide for good and bad workloads. Don't use multi-node concurrent file access . Do collection of data on the job's master node and write data to disk only from the master node. Don't send a ticket just because this ls -l doesn't finish in 5s. A slow response is almost certainly not indicating a malfunction of the ceph cluster, but rather a bad workload like concurrent access of a directory sub-tree. Don't run a job and check its progress with ls -l on the head node. This creates a concurrent access with all the synchronization overhead. It will not only be slow on the head node, it will also stall the job. Do ssh/mrsh into the master node of the job and run your ls -l there (i.e. use single-node access). Don't submit a sequence of jobs using the same application. After each job the SLURM prologue script flushes the file system cache on the compute nodes. To avoid reloading the same data from disk for every computation, do run a sequence of computations with the same application in one job. This is particularly important for applications that open thousands of files, like Matlab. Use srun to execute single-application multiple-input jobs in an efficient way. Don't call fflush and fsync , sync excessively, for example, after every single write. Let writes accummulate in the system buffer and call one of these functions after at least 4MB worth of writes are processed. Do use fclose() after writes are completed to ensure data is committed .","title":"Performance characteristics, Do's and Dont's"},{"location":"permanent/#snapshots","text":"In case of accidental deletion, corruption or modification of data by malware, previous versions are available in ceph snapshots. To access a snapshot of a file, change to the file's directory and execute cd .snap . Note that \".snap\" is a meta-directory, not a real directory. Therefore, tab-completion and the like will not work and one needs to type it explicitly as stated here. Inside the snapshot directory are folders with current snapshots, for example, [frans@sophia1 .snap]$ ls _2021-02-23_183554+0100_weekly_1099511719926 _2021-02-27_000611+0100_daily_1099511719926 _2021-02-24_000611+0100_daily_1099511719926 _2021-02-28_000611+0100_daily_1099511719926 _2021-02-25_000611+0100_daily_1099511719926 _2021-03-01_000611+0100_daily_1099511719926 _2021-02-26_000611+0100_daily_1099511719926 _2021-03-01_000911+0100_weekly_1099511719926 To access snapshots of a specific date, cd into the corresponding folder and browse through the files. Note that files in snapshots are read-only regardless of the file permissions shown by ls -l . Disclaimer Snapshots are provided as a courtesy service on the ceph file systems /home and /groups . The availability of snapshots depends on the amount of currently unused ceph storage capacity and the retention time may be reduced without warning to adjust to increased utilisation. The default retention scheme is: daily snapshots for 7 days and weekly snapshots for 4 weeks. We cannot guarantee snapshots to be present in situations where excessive amounts of temporary data have been captured in snapshots. We aim, however, to have at least the last two days available.","title":"Snapshots"},{"location":"permanent/#advanced-topics","text":"LazyIO provided by libcephfs Ceph fs and POSIX .","title":"Advanced topics"},{"location":"scheduler/","text":"The Slurm job scheduler Sophia uses the Slurm cluster management and job scheduling system . Computations on Sophia are executed through interactive Slurm sessions or via batch compute job scripts. Job queues a.k.a. partitions Usage of particular computation resources available on Sophia is governed by specification of a job queue , or partition in Slurm lingo, which can be done either on the commandline as argument to the srun command or via a batch job script that is submitted to the scheduler with the sbatch command. Sophia compute nodes are organised in the following job queues/partitions: Name Open for all Sophia users workq AMD EPYC 7351 (1st gen, 32 cores), 128 GB memory romeq AMD EPYC 7302 (2nd gen, 32 cores), 128 GB memory fatq AMD EPYC 7351, 256 GB memory gpuq 1 Nvidia Quadro P4000 GPU per node v100 1 Nvidia Tesla V100 per node Name Exclusive access for DTU Wind Energy staff windq AMD EPYC 7351 (1st gen, 32 cores), 128 GB memory windfatq AMD EPYC 7351 (1st gen, 32 cores), 256 GB memory Use the sinfo command to list information about the Slurm partitions configured and squeue to list compute jobs on queue. Interactive jobs For code testing purposes an interactive terminal session is convenient and can be requested using the srun command. The following example illustrates the procedure; srun --partition windq --time 06:00:00 --nodes 2 --ntasks-per-node 32 --pty bash which requests an interactive job with 2 compute nodes from the windq partition, and all 32 physical cores available on each, for 6 hours. The session is granted when resources become available and ends when the user exits the terminal or once the time limit - here 6 hours - is reached. Batch jobs Once the simulation workflow has been tested via interactive jobs one may wish to run a number of jobs unsupervised. To dispatch jobs programmatically a job script must be prepared; Slurm job script example [<username>@sophia1 ~]$ cat slurm.job #!/bin/bash #SBATCH --time=2:00:00 #SBATCH --partition=workq #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 echo \"hello, Sophia!\" which can then be submitted with the sbatch command, [<username>@sophia1 ~]$ sbatch slurm.job","title":"Job scheduler"},{"location":"scheduler/#the-slurm-job-scheduler","text":"Sophia uses the Slurm cluster management and job scheduling system . Computations on Sophia are executed through interactive Slurm sessions or via batch compute job scripts.","title":"The Slurm job scheduler"},{"location":"scheduler/#job-queues-aka-partitions","text":"Usage of particular computation resources available on Sophia is governed by specification of a job queue , or partition in Slurm lingo, which can be done either on the commandline as argument to the srun command or via a batch job script that is submitted to the scheduler with the sbatch command. Sophia compute nodes are organised in the following job queues/partitions: Name Open for all Sophia users workq AMD EPYC 7351 (1st gen, 32 cores), 128 GB memory romeq AMD EPYC 7302 (2nd gen, 32 cores), 128 GB memory fatq AMD EPYC 7351, 256 GB memory gpuq 1 Nvidia Quadro P4000 GPU per node v100 1 Nvidia Tesla V100 per node Name Exclusive access for DTU Wind Energy staff windq AMD EPYC 7351 (1st gen, 32 cores), 128 GB memory windfatq AMD EPYC 7351 (1st gen, 32 cores), 256 GB memory Use the sinfo command to list information about the Slurm partitions configured and squeue to list compute jobs on queue.","title":"Job queues a.k.a. partitions"},{"location":"scheduler/#interactive-jobs","text":"For code testing purposes an interactive terminal session is convenient and can be requested using the srun command. The following example illustrates the procedure; srun --partition windq --time 06:00:00 --nodes 2 --ntasks-per-node 32 --pty bash which requests an interactive job with 2 compute nodes from the windq partition, and all 32 physical cores available on each, for 6 hours. The session is granted when resources become available and ends when the user exits the terminal or once the time limit - here 6 hours - is reached.","title":"Interactive jobs"},{"location":"scheduler/#batch-jobs","text":"Once the simulation workflow has been tested via interactive jobs one may wish to run a number of jobs unsupervised. To dispatch jobs programmatically a job script must be prepared; Slurm job script example [<username>@sophia1 ~]$ cat slurm.job #!/bin/bash #SBATCH --time=2:00:00 #SBATCH --partition=workq #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 echo \"hello, Sophia!\" which can then be submitted with the sbatch command, [<username>@sophia1 ~]$ sbatch slurm.job","title":"Batch jobs"},{"location":"scratch/","text":"Temporary storage performance-hierachy The temporary storage options are listed below; fastest first. Each compute node's local /tmp directory Sophia compute nodes are ephemeral meaning they fetch a minimal CentOS installation at each reboot which then runs in the node's memory. This means that the underlying XFS file system runs on the compute node's random access memory hardware with orders of magnitude higher bandwidth and lower latency compared with disk drives. Thus, e.g. to eliminate I/O as the limiting factor in a application performance benchmark experiment, a Sophia user can run test code from the /tmp directory on up to 32 MPI processes on a single Sophia node with 32 physical cores. Burst buffer Two servers with NVMe disks - Sophia's burst buffer nodes - are connected to Sophia compute nodes via the HPC cluster's Mellanox EDR Infiniband interconnect, and each server connects to the Ceph file system via bonded 10Gbps connections (i.e. 20Gbps). Sophia's burst buffer runs the BeeGFS file system , striping data written from compute nodes across BeeGFS storage targets. The storage resource is mounted on /mnt/beegfs on Sophia's head node and all compute nodes and current usage is manual. Remember to clean up after use since lingering data is removed periodically.","title":"Generating"},{"location":"scratch/#temporary-storage-performance-hierachy","text":"The temporary storage options are listed below; fastest first.","title":"Temporary storage performance-hierachy"},{"location":"scratch/#each-compute-nodes-local-tmp-directory","text":"Sophia compute nodes are ephemeral meaning they fetch a minimal CentOS installation at each reboot which then runs in the node's memory. This means that the underlying XFS file system runs on the compute node's random access memory hardware with orders of magnitude higher bandwidth and lower latency compared with disk drives. Thus, e.g. to eliminate I/O as the limiting factor in a application performance benchmark experiment, a Sophia user can run test code from the /tmp directory on up to 32 MPI processes on a single Sophia node with 32 physical cores.","title":"Each compute node's local /tmp directory"},{"location":"scratch/#burst-buffer","text":"Two servers with NVMe disks - Sophia's burst buffer nodes - are connected to Sophia compute nodes via the HPC cluster's Mellanox EDR Infiniband interconnect, and each server connects to the Ceph file system via bonded 10Gbps connections (i.e. 20Gbps). Sophia's burst buffer runs the BeeGFS file system , striping data written from compute nodes across BeeGFS storage targets. The storage resource is mounted on /mnt/beegfs on Sophia's head node and all compute nodes and current usage is manual. Remember to clean up after use since lingering data is removed periodically.","title":"Burst buffer"},{"location":"sharing/","text":"Private and shared On the Sophia storage system there are two types of directories; subdirectories for single-user data are under /home , whereas subdirectories for multi-user data are under /groups . private data shared data /home/<dtu user> /groups/<group name> Multi-user directories must never contain personal user data (ssh keys, etc.). If you would like to have a group directory created please write the address at the bottom left.","title":"Private and shared"},{"location":"sharing/#private-and-shared","text":"On the Sophia storage system there are two types of directories; subdirectories for single-user data are under /home , whereas subdirectories for multi-user data are under /groups . private data shared data /home/<dtu user> /groups/<group name> Multi-user directories must never contain personal user data (ssh keys, etc.). If you would like to have a group directory created please write the address at the bottom left.","title":"Private and shared"},{"location":"subsystems/","text":"Sophia cluster systems The Sophia HPC cluster consists of login- and compute nodes burst buffer nodes for temporary data each node with fast interconnect fabric . Embedded in the racks, alongside Sophia compute- and burst buffer nodes, is a storage cluster for permanent data hosting the $HOME directory for Sophia users as well as shared directory infrastructure to facilitate collaboration on projects. The storage cluster is connected to compute nodes and burst buffer with high-end commodity interconnect fabric .","title":"Cluster systems"},{"location":"subsystems/#sophia-cluster-systems","text":"The Sophia HPC cluster consists of login- and compute nodes burst buffer nodes for temporary data each node with fast interconnect fabric . Embedded in the racks, alongside Sophia compute- and burst buffer nodes, is a storage cluster for permanent data hosting the $HOME directory for Sophia users as well as shared directory infrastructure to facilitate collaboration on projects. The storage cluster is connected to compute nodes and burst buffer with high-end commodity interconnect fabric .","title":"Sophia cluster systems"},{"location":"transfer/","text":"Data transfer In this section methods for moving data to and from Sophia are described. Using sshfs Secure SHell FileSystem (SSHFS) allows to mount remote locations on a local machine. Any user storage resources reachable with ssh can be mounted as a file system. Since SSHFS creates a user space file system, administrator privileges are not required. sshfs on Linux laptop Install via your package manager or build from source . Then identify Linux id for your laptop user and primary group $ id $USER which are the uid and gid numbers in the command's output. On your laptop, create a mount path owned by your local user, e.g. sudo mkdir -p /mnt/sophia/$USER sudo chown $USER.$USER /mnt/sophia/$USER and e.g. mount your Sophia $HOME path with sshfs -o uid=<local user uid>,gid=<local user gid> <dtu user>@sophia.dtu.dk:/home/<dtu user> /mnt/sophia/<local user name> sshfs on Windows laptop There are a number of options , e.g. Install latest WinSFP from github Install latest SSHFS-Win from github Open File Explorer, right -click on This PC and choose Map network drive . Choose a drive to mount at and in the Folder field enter \\\\sshfs\\<dtu user>@sophia.dtu.dk to mount your Sophia $HOME directory. Using sftp FileZilla This is probably one of the easiest and fastest way to transfer files. FileZilla allows for recursive file transfer, concurrent file transfers, uses incremental file transfer and allows to resume interrupted file transfers. FileZilla is a file transfer client, which runs under Windows, Linux, Mac/IOS and other systems. It is able to transfer files via sftp and ssh file transfer protocols. For most Linux distributions pre-compiled packages are available. The HPC clusters support the ssh file transfer protocol. The set-up is very simple: Commands are shown for the Ubuntu operating system and connecting to Jess. Install FileZilla. Start FileZilla and open the site manager under File->Site Manager (or press +s). Create a new site with name Sophia. In the general tab, enter: Host: sophia.dtu.dk Protocol: SFTP - SSH File Transfer protocol Logon Type: Ask for Password User: <dtu user> Click connect to test the connection. A particularly nice feature of FileZilla is synchronized browsing , which is useful for keeping entire directory trees in sync. It also allows editing of remote files from the local computer.","title":"Moving"},{"location":"transfer/#data-transfer","text":"In this section methods for moving data to and from Sophia are described.","title":"Data transfer"},{"location":"transfer/#using-sshfs","text":"Secure SHell FileSystem (SSHFS) allows to mount remote locations on a local machine. Any user storage resources reachable with ssh can be mounted as a file system. Since SSHFS creates a user space file system, administrator privileges are not required.","title":"Using sshfs"},{"location":"transfer/#sshfs-on-linux-laptop","text":"Install via your package manager or build from source . Then identify Linux id for your laptop user and primary group $ id $USER which are the uid and gid numbers in the command's output. On your laptop, create a mount path owned by your local user, e.g. sudo mkdir -p /mnt/sophia/$USER sudo chown $USER.$USER /mnt/sophia/$USER and e.g. mount your Sophia $HOME path with sshfs -o uid=<local user uid>,gid=<local user gid> <dtu user>@sophia.dtu.dk:/home/<dtu user> /mnt/sophia/<local user name>","title":"sshfs on Linux laptop"},{"location":"transfer/#sshfs-on-windows-laptop","text":"There are a number of options , e.g. Install latest WinSFP from github Install latest SSHFS-Win from github Open File Explorer, right -click on This PC and choose Map network drive . Choose a drive to mount at and in the Folder field enter \\\\sshfs\\<dtu user>@sophia.dtu.dk to mount your Sophia $HOME directory.","title":"sshfs on Windows laptop"},{"location":"transfer/#using-sftp","text":"","title":"Using sftp"},{"location":"transfer/#filezilla","text":"This is probably one of the easiest and fastest way to transfer files. FileZilla allows for recursive file transfer, concurrent file transfers, uses incremental file transfer and allows to resume interrupted file transfers. FileZilla is a file transfer client, which runs under Windows, Linux, Mac/IOS and other systems. It is able to transfer files via sftp and ssh file transfer protocols. For most Linux distributions pre-compiled packages are available. The HPC clusters support the ssh file transfer protocol. The set-up is very simple: Commands are shown for the Ubuntu operating system and connecting to Jess. Install FileZilla. Start FileZilla and open the site manager under File->Site Manager (or press +s). Create a new site with name Sophia. In the general tab, enter: Host: sophia.dtu.dk Protocol: SFTP - SSH File Transfer protocol Logon Type: Ask for Password User: <dtu user> Click connect to test the connection. A particularly nice feature of FileZilla is synchronized browsing , which is useful for keeping entire directory trees in sync. It also allows editing of remote files from the local computer.","title":"FileZilla"}]}